{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GVSU-CIS635/term-project-proposal-data_minds/blob/main/Credit_Card_Fraud_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNUaNXAA-GWj"
      },
      "source": [
        "# Credit Card Fraud Detection\n",
        "\n",
        "## Context Summary\n",
        "\n",
        "The dataset represents credit card transactions made by European cardholders over two days in September 2013. Out of 284,807 transactions, 492 are fraudulent, making this a highly imbalanced dataset with the positive class (fraud) representing only 0.172% of the total. The features have been anonymized using Principal Component Analysis (PCA) to protect confidentiality, except for the `Time` and `Amount` columns.\n",
        "\n",
        "### Key Features\n",
        "\n",
        "1. **Time**:\n",
        "    \n",
        "    - Measures the seconds elapsed from the first transaction in the dataset.\n",
        "    - Can help identify patterns such as the frequency of transactions over time, which may be useful in fraud detection (e.g., unusually rapid transactions could indicate fraud).\n",
        "2. **V1, V2, ..., V28**:\n",
        "    \n",
        "    - These are PCA-transformed features. Each component captures a combination of characteristics from the original transaction data that contribute most to variance.\n",
        "    - Due to the transformation, they donâ€™t have direct interpretations, but the model can still learn patterns from these features.\n",
        "3. **Amount**:\n",
        "    \n",
        "    - Represents the monetary value of the transaction.\n",
        "    - Can be used for cost-sensitive learning; larger transaction amounts could be a red flag when associated with other suspicious behaviors.\n",
        "4. **Class**:\n",
        "    \n",
        "    - The target variable:\n",
        "        - **0**: Legitimate transaction.\n",
        "        - **1**: Fraudulent transaction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHqGlBxbBzT-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, RocCurveDisplay, precision_recall_curve, auc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from xgboost import XGBClassifier\n",
        "%matplotlib inline\n",
        "from matplotlib.pylab import rcParams\n",
        "import zipfile\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import textwrap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5InFeDfHRGd"
      },
      "source": [
        "## Section 1: Implementation Plan\n",
        "\n",
        "Let's explore the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGC3m2pSCXXp"
      },
      "outputs": [],
      "source": [
        "# Download the zip file from the URL\n",
        "url = 'https://github.com/GVSU-CIS635/Datasets/blob/master/creditcard.csv.zip?raw=true'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Define the new column names\n",
        "new_column_names = []\n",
        "\n",
        "# Rename the PCA-transformed features (V1 to V28) dynamically\n",
        "for i in range(1, 29):\n",
        "    new_column_names.append(f\"V{i}\")\n",
        "\n",
        "# Rename the columns in the dataset\n",
        "new_column_names = [\"Time\"] + new_column_names + [\"Amount\", \"Class\"]\n",
        "\n",
        "# Open the zip file from the downloaded content\n",
        "with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
        "    # Choose the file within the zip (e.g., 'data.csv')\n",
        "    file_name = 'creditcard.csv'  # Replace with the name of the CSV file inside the zip\n",
        "\n",
        "    # Read the CSV file directly from the zip archive\n",
        "    with zip_ref.open(file_name) as file:\n",
        "        data = pd.read_csv(file, names=new_column_names)\n",
        "\n",
        "# Display basic information about the dataset and the first few rows to understand its structure\n",
        "data_info = data.info()\n",
        "data_head = data.head()\n",
        "\n",
        "display(data_info)\n",
        "display(data_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSAj71LCueEi"
      },
      "source": [
        "### Step 1: Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AG4GwAWD8DU"
      },
      "source": [
        "- **Handle Missing Values**: Check for any missing values to maintain data quality and ensure model training isn't disrupted by null values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCLj7zmpC8Q5"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values in the dataset:\\n\", data.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5sDRtfnEWkO"
      },
      "source": [
        "- **Scale \"Amount\" Feature**: The 'Amount' feature might have values that differ significantly in scale compared to other features. Scaling ensures uniformity and prevents large values from disproportionately impacting the model's learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFnKxNnKEZar"
      },
      "outputs": [],
      "source": [
        "# Scale the 'Amount' feature\n",
        "scaler = StandardScaler()\n",
        "data['Amount_Scaled'] = scaler.fit_transform(data[['Amount']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeXieAeLFu8B"
      },
      "source": [
        "- **Analyze the Distribution of \"Time\"**: Visualize the distribution of the 'Time' feature to understand transaction patterns that could affect model training.\n",
        "  - From the output, we can deduce that:\n",
        "    - The correlation between \"Time\" and \"Class\" (fraud label) is approximately -0.012, which is very close to zero. This indicates a negligible relationship between the transaction time and the likelihood of fraud.\n",
        "    - The \"Time\" has very low correlation with \"Class\" fraud attribute, indicating it may have limited predictive value for fraud detection on its own. However, it could still contribute to the model in combination with other features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK1fUWZmFyBk"
      },
      "outputs": [],
      "source": [
        "# Plotting distribution of fraudulent and non-fraudulent transactions over time\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot non-fraudulent transactions over time\n",
        "sns.histplot(data[data['Class'] == 0]['Time'], bins=50, color='blue', label='Non-Fraud', kde=True, alpha=0.6)\n",
        "\n",
        "# Plot fraudulent transactions over time\n",
        "sns.histplot(data[data['Class'] == 1]['Time'], bins=50, color='red', label='Fraud', kde=True, alpha=0.6)\n",
        "\n",
        "# Plot styling\n",
        "plt.title('Distribution of Fraudulent and Non-Fraudulent Transactions Over Time')\n",
        "plt.xlabel('Time (seconds since first transaction)')\n",
        "plt.ylabel('Transaction Count')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Calculate correlation between 'Time' and 'Class' to quantify the relationship\n",
        "correlation_time_fraud = data[['Time', 'Class']].corr().iloc[0, 1]\n",
        "print(f'Correlation between Time and Class: {correlation_time_fraud:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwS37g3zKWTZ"
      },
      "source": [
        "- **Separate Features and Target**:\n",
        "  - Split the data into features (X) and the target variable (y) to prepare for model training. The target variable ('0.1') indicates whether a transaction is fraudulent (1) or not (0).\n",
        "  - Create a copy of the dataset before dropping columns to preserve the original data for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-D77tl8KxbK"
      },
      "outputs": [],
      "source": [
        "# Drop the original 'Amount' column from the copied DataFrame, since we only need the 'Amount_Scaled' column\n",
        "data = data.drop(['Amount'], axis=1)\n",
        "\n",
        "# Prepare features (X) and target (y)\n",
        "X = data.drop(['Class'], axis=1)  # Features\n",
        "y = data['Class']  # Target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytxtC5pIL84J"
      },
      "source": [
        "- **Resampling Using SMOTE**:\n",
        "\n",
        "  - The dataset is highly imbalanced, with many more legitimate transactions than fraudulent ones.\n",
        "  - To address this, SMOTE is applied to the training data only to create synthetic samples of the minority class, balancing the training set and improving the model's ability to learn patterns for fraud detection.\n",
        "  - The test set remains untouched to ensure realistic and unbiased evaluation of the model's performance on data with its original class distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRyzUY07Ml0m"
      },
      "outputs": [],
      "source": [
        "# Print class counts before SMOTE for clarity\n",
        "print(\"Class distribution before SMOTE:\")\n",
        "print(y.value_counts())\n",
        "\n",
        "# Split the resampled data into training and testing sets (70-30 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply SMOTE only to the training data to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print class counts after SMOTE for clarity\n",
        "print(\"\\nClass distribution after SMOTE:\")\n",
        "print(y_train_resampled.value_counts())\n",
        "\n",
        "# Check original class distribution\n",
        "original_class_counts = y.value_counts()\n",
        "\n",
        "# Check new class distribution after SMOTE\n",
        "resampled_class_counts = pd.Series(y_train_resampled).value_counts()\n",
        "\n",
        "# Define consistent colors for classes\n",
        "class_colors = {0: 'blue', 1: 'cyan'}\n",
        "\n",
        "# Plot class distributions before and after SMOTE\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
        "\n",
        "# Before SMOTE\n",
        "axes[0].bar(original_class_counts.index, original_class_counts.values, color=[class_colors[cls] for cls in original_class_counts.index])\n",
        "axes[0].set_title('Before SMOTE')\n",
        "axes[0].set_xticks([0, 1])\n",
        "axes[0].set_xticklabels(['Legitimate transactions (0)', 'Fraudulent transactions (1)'])\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].grid(axis='y')\n",
        "\n",
        "# After SMOTE\n",
        "axes[1].bar(resampled_class_counts.index, resampled_class_counts.values, color=[class_colors[cls] for cls in resampled_class_counts.index])\n",
        "axes[1].set_title('After SMOTE')\n",
        "axes[1].set_xticks([0, 1])\n",
        "axes[1].set_xticklabels(['Legitimate transactions (0)', 'Fraudulent transactions (1)'])\n",
        "axes[1].grid(axis='y')\n",
        "\n",
        "plt.suptitle('Class Distribution Before and After SMOTE')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_wD8oiJP_U-"
      },
      "source": [
        "### Step 2: Model Development\n",
        "\n",
        "- **Training Models**:\n",
        "  - Splitting the dataset into training and testing sets allows us to train the model on one portion and evaluate it on another, ensuring generalizability and preventing overfitting.\n",
        "  - Training multiple models ensures we explore different machine learning algorithms to find the most effective one for classifying fraudulent transactions.\n",
        "  - Using cross-validation helps validate the model's performance across different subsets of the data, and GridSearchCV helps fine-tune hyperparameters to optimize model performance.\n",
        "\n",
        "- **Models Used**:\n",
        "    \n",
        "    - **Logistic Regression**: A baseline linear model for binary classification.\n",
        "    - **Random Forest**: An ensemble of decision trees for better accuracy and robustness.\n",
        "    - **XGBoost**: A powerful gradient boosting algorithm that often yields high performance.\n",
        "    - **Neural Network**: A multi-layer perceptron to capture complex patterns in data.\n",
        "\n",
        "- **Hyperparameter Tuning**:\n",
        "\n",
        "    - **`GridSearchCV`**: Used for an exhaustive search over the specified parameter grids to find the best hyperparameters.\n",
        "    - **`StratifiedKFold`**: Ensures each fold in cross-validation has a representative proportion of each class, providing a more reliable performance measure.\n",
        "    - **Scoring Metric**: The `roc_auc` score is used as the scoring metric, suitable for imbalanced classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qdhqOhLQEvV",
        "outputId": "925af9c6-6556-4cfd-f310-3bf66183e081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
          ]
        }
      ],
      "source": [
        "# Define models to train\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=10000, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'XGBoost': XGBClassifier(eval_metric='logloss', random_state=42),\n",
        "    'Neural Network': MLPClassifier(max_iter=500, random_state=42)\n",
        "}\n",
        "\n",
        "# Define parameter grids for each model to tune hyperparameters\n",
        "param_grid = {\n",
        "    'Logistic Regression': {'C': [0.01, 0.1, 1, 10]},\n",
        "    'Random Forest': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]},\n",
        "    'XGBoost': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 6, 10]},\n",
        "    'Neural Network': {'hidden_layer_sizes': [(50,), (100,), (100, 50)], 'activation': ['relu', 'tanh']}\n",
        "}\n",
        "\n",
        "# Dictionary to store the best models\n",
        "best_models = {}\n",
        "\n",
        "# Train models with cross-validation and GridSearchCV for hyperparameter tuning\n",
        "for model_name, model in models.items():\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grid[model_name],\n",
        "        cv=StratifiedKFold(5),\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=-1,\n",
        "        verbose=4\n",
        "    )\n",
        "    grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "    best_models[model_name] = grid_search.best_estimator_\n",
        "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
        "\n",
        "print(best_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Model Evaluation\n",
        "\n",
        "- After tuning and selecting the best model parameters in the `best_models` dictionary, we will evaluate each model on the test set:\n",
        "    - **Classification Report**: Precision, recall, F1-score, and accuracy.\n",
        "    - **ROC-AUC Score** and **ROC Curve**: Indicates the model's discriminative ability.\n",
        "    - **Confusion Matrix**: Visualizes true positives, false positives, true negatives, and false negatives, highlighting model accuracy on fraud detection.\n",
        "    - **AUPRC Score and Precision-Recall Curve**: Especially useful for imbalanced datasets, as it shows the balance between precision and recall."
      ],
      "metadata": {
        "id": "4h8LM47qIQiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate and print model performance\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of a classification model and provides textual metrics\n",
        "    and visualizations including ROC Curve, Precision-Recall Curve, and Confusion Matrix.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    model : sklearn-like estimator\n",
        "        The trained model to evaluate. Must implement `predict` and optionally `predict_proba`\n",
        "        for probability predictions.\n",
        "    X_test : array-like or DataFrame\n",
        "        The feature set used for testing.\n",
        "    y_test : array-like\n",
        "        The true labels corresponding to the test set.\n",
        "    model_name : str\n",
        "        A string representing the name of the model, used for titles and display.\n",
        "\n",
        "    Returns:\n",
        "    ------\n",
        "    Void\n",
        "    \"\"\"\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
        "\n",
        "    # Print classification report\n",
        "    print('*************************************************************************************')\n",
        "    print(f\"--- {model_name} Evaluation ---\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Start a figure with three subplots for ROC curve, Precision-Recall curve, and Confusion Matrix\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    # Plot ROC Curve if probabilities are available\n",
        "    if y_pred_proba is not None:\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "        # Plot ROC curve\n",
        "        RocCurveDisplay.from_estimator(model, X_test, y_test, ax=axes[0])\n",
        "        wrapped_title_roc = \"\\n\".join(textwrap.wrap(f'ROC Curve for {model_name}', width=30)) # Wrap title\n",
        "        axes[0].set_title(wrapped_title_roc)\n",
        "\n",
        "        # Calculate and plot Precision-Recall Curve for AUPRC if probabilities are available\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "        auprc = auc(recall, precision)\n",
        "        print(f\"AUPRC Score: {auprc:.4f}\")\n",
        "\n",
        "        # Plot Precision-Recall Curve\n",
        "        axes[1].plot(recall, precision, label=f'AUPRC = {auprc:.4f}')\n",
        "        axes[1].set_xlabel('Recall')\n",
        "        axes[1].set_ylabel('Precision')\n",
        "        wrapped_title_pr = \"\\n\".join(textwrap.wrap(f'Precision-Recall Curve for {model_name}', width=30)) # Wrap title\n",
        "        axes[1].set_title(wrapped_title_pr)\n",
        "        axes[1].legend(loc='best')\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Not Fraud', 'Fraud'],\n",
        "                yticklabels=['Not Fraud', 'Fraud'],\n",
        "                ax=axes[2])\n",
        "    axes[2].set_xlabel('Predicted')\n",
        "    axes[2].set_ylabel('Actual')\n",
        "    wrapped_title_cm = \"\\n\".join(textwrap.wrap(f'Confusion Matrix for {model_name}', width=30)) # Wrap title\n",
        "    axes[2].set_title(wrapped_title_cm)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate each model in best_models on the test set\n",
        "for model_name, model in best_models.items():\n",
        "    evaluate_model(model, X_test, y_test, model_name)\n"
      ],
      "metadata": {
        "id": "c2yofZIoL8_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Ensemble Learning\n",
        "\n",
        "- `Random Forest` and `XGBoost` are retrieved from the `best_models` dictionary and used to create the `VotingClassifier`.\n",
        "- We use **soft voting** to combine the models, where the Voting Classifier averages the predicted probabilities from both `Random Forest` and `XGBoost`. This approach can often lead to more balanced and robust predictions.\n",
        "- We use the `evaluate_model` function to assess the ensemble model's performance, displaying:\n",
        "    - **Classification Report**: Precision, recall, F1-score, and accuracy.\n",
        "    - **ROC-AUC Score** and **ROC Curve**: Indicates the model's discriminative ability.\n",
        "    - **Confusion Matrix**: Visualizes true positives, false positives, true negatives, and false negatives, highlighting model accuracy on fraud detection.\n",
        "    - **AUPRC Score and Precision-Recall Curve**: Especially useful for imbalanced datasets, as it shows the balance between precision and recall."
      ],
      "metadata": {
        "id": "726DxHTNWvss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the saved models from the best_models dictionary\n",
        "random_forest = best_models['Random Forest']\n",
        "xgboost = best_models['XGBoost']\n",
        "\n",
        "# Create the Voting Classifier using the saved models with Soft Voting\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('Random Forest', random_forest), ('XGBoost', xgboost)],\n",
        "    voting='soft'  # Soft voting uses the average of predicted probabilities\n",
        ")\n",
        "\n",
        "# Fit the Voting Classifier to the training data (if necessary)\n",
        "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Evaluate the Voting Classifier\n",
        "evaluate_model(voting_clf, X_test, y_test, \"Voting Classifier (Random Forest + XGBoost)\")"
      ],
      "metadata": {
        "id": "3Z8wzBu_OvVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Evaluation Plan\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation of Evaluation Metrics\n",
        "\n",
        "1. **Classification Report**:\n",
        "   - Provides metrics like **Precision**, **Recall**, and **F1-Score** for each class (Not Fraud and Fraud).\n",
        "   - These metrics are especially important for fraud detection, where correctly identifying the minority class (Fraud) is crucial.\n",
        "   - **Precision** indicates the percentage of transactions classified as fraud that are actually fraud, while **Recall** (True Positive Rate) measures the percentage of actual fraud cases that were successfully identified.\n",
        "\n",
        "2. **AUPRC (Area Under the Precision-Recall Curve)**:\n",
        "   - **AUPRC** is well-suited for imbalanced datasets because it focuses on the positive (fraud) class.\n",
        "   - The **Precision-Recall Curve** shows the trade-off between precision and recall at various thresholds, and AUPRC measures the area under this curve.\n",
        "   - **Interpretation**: A higher AUPRC score indicates that the model maintains high precision and recall simultaneously, which is essential in fraud detection where correctly identifying fraud is crucial.\n",
        "\n",
        "3. **ROC-AUC Score**:\n",
        "   - The ROC-AUC score measures the model's ability to distinguish between classes across various thresholds.\n",
        "   - A higher ROC-AUC score indicates better overall performance in separating fraud from non-fraud, especially helpful in imbalanced datasets.\n",
        "\n",
        "4. **ROC Curve**:\n",
        "   - The ROC curve visually shows the trade-off between the **True Positive Rate** (Recall) and **False Positive Rate** across different thresholds.\n",
        "   - This helps assess the model's separability of classes, which is useful in determining how well it can identify fraud without flagging too many false positives.\n",
        "\n",
        "5. **Confusion Matrix**:\n",
        "   - Displays counts of **True Positives** (fraud correctly identified), **False Positives** (non-fraud classified as fraud), **True Negatives** (non-fraud correctly identified), and **False Negatives** (fraud missed).\n",
        "   - Provides insight into the model's classification accuracy, especially for understanding misclassification patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### What to Look For\n",
        "\n",
        "1. **High Recall for the Fraud Class**:\n",
        "   - Since this is a fraud detection problem, **high Recall** for the Fraud class is typically more important than overall accuracy. This ensures that most fraudulent transactions are detected.\n",
        "\n",
        "2. **Balanced Precision and Recall**:\n",
        "   - A balance between **Precision** and **Recall** (i.e., a high F1-score) for the Fraud class is desirable to minimize both False Negatives and False Positives.\n",
        "\n",
        "3. **High AUPRC**:\n",
        "   - A high AUPRC score suggests that the model performs well at both high and low thresholds for fraud detection, balancing precision and recall effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### Combined Model Evaluation and Analysis\n",
        "\n",
        "Each model is evaluated based on **Precision**, **Recall**, **F1-Score**, **AUPRC**, **ROC-AUC**, and **Confusion Matrix**, with a specific focus on identifying fraudulent transactions.\n",
        "\n",
        "#### **1. Logistic Regression**\n",
        "\n",
        "**Performance Metrics:**\n",
        "\n",
        "- **Precision for fraud**: 0.14 - A relatively low precision, indicating a higher proportion of legitimate transactions are misclassified as fraud.\n",
        "- **Recall for fraud**: 0.93 - The model successfully identifies 93% of fraudulent transactions, minimizing false negatives.\n",
        "- **F1-Score**: 0.24 - Reflects the balance between precision and recall but is limited due to low precision.\n",
        "- **ROC-AUC Score**: 0.9825 - A high score indicating good overall discrimination between classes.\n",
        "- **AUPRC Score**: 0.8086 - A strong balance of precision and recall for identifying fraud.\n",
        "\n",
        "**Visual Interpretation:**\n",
        "\n",
        "- **ROC Curve**: Approaches the top-left corner, showing strong sensitivity and specificity.\n",
        "- **Precision-Recall Curve**: High recall but lower precision due to more false positives.\n",
        "- **Confusion Matrix**: Indicates excellent fraud detection (high recall) but misclassifies some legitimate transactions as fraud.\n",
        "\n",
        "**Summary:** Logistic Regression achieves high recall for fraud cases, making it suitable for situations where detecting fraud is critical. However, the low precision could lead to operational challenges from excessive false positives.\n",
        "\n",
        "\n",
        "#### **2. Random Forest**\n",
        "\n",
        "**Performance Metrics:**\n",
        "\n",
        "- **Precision for fraud**: 0.80 - Indicates most flagged fraudulent transactions are correctly identified.\n",
        "- **Recall for fraud**: 0.88 - Detects a high percentage of fraudulent transactions with relatively few false negatives.\n",
        "- **F1-Score**: 0.84 - Reflects strong overall performance with a good balance between precision and recall.\n",
        "- **ROC-AUC Score**: 0.9900 - Demonstrates excellent discrimination between fraud and non-fraud classes.\n",
        "- **AUPRC Score**: 0.8810 - A well-balanced score emphasizing reliable fraud detection.\n",
        "\n",
        "**Visual Interpretation:**\n",
        "\n",
        "- **ROC Curve**: Approaches the top-left corner, highlighting high sensitivity and specificity.\n",
        "- **Precision-Recall Curve**: Balanced curve close to the top-right corner, showing strong performance across thresholds.\n",
        "- **Confusion Matrix**: Few false positives and false negatives, showing robust fraud detection.\n",
        "\n",
        "**Summary:** Random Forest provides strong, balanced performance for fraud detection, making it a reliable choice. Its high precision and recall suggest fewer operational challenges compared to Logistic Regression.\n",
        "\n",
        "\n",
        "#### **3. XGBoost**\n",
        "\n",
        "**Performance Metrics:**\n",
        "\n",
        "- **Precision for fraud**: 0.78 - Slightly lower than Random Forest, but still high.\n",
        "- **Recall for fraud**: 0.86 - Captures a large proportion of fraud cases, though slightly fewer than Random Forest.\n",
        "- **F1-Score**: 0.82 - A balanced score reflecting strong performance.\n",
        "- **ROC-AUC Score**: 0.9911 - Marginally better than Random Forest, indicating excellent discrimination.\n",
        "- **AUPRC Score**: 0.8858 - Strong balance between precision and recall, slightly outperforming Random Forest.\n",
        "\n",
        "**Visual Interpretation:**\n",
        "\n",
        "- **ROC Curve**: Near the top-left corner, demonstrating high sensitivity and specificity.\n",
        "- **Precision-Recall Curve**: Strong curve, balancing precision and recall effectively.\n",
        "- **Confusion Matrix**: Similar to Random Forest, with minimal misclassifications.\n",
        "\n",
        "**Summary:** XGBoost offers performance close to Random Forest with marginally better ROC-AUC and AUPRC scores. It is a strong candidate for fraud detection but should be validated further to confirm generalizability.\n",
        "\n",
        "\n",
        "#### **4. Neural Network**\n",
        "\n",
        "**Performance Metrics:**\n",
        "\n",
        "- **Precision for fraud**: 0.18 - Indicates many false positives.\n",
        "- **Recall for fraud**: 0.90 - Effectively identifies most fraudulent transactions but with a trade-off in precision.\n",
        "- **F1-Score**: 0.30 - Reflects a significant gap between precision and recall.\n",
        "- **ROC-AUC Score**: 0.9684 - Lower than other models, indicating slightly reduced discrimination.\n",
        "- **AUPRC Score**: 0.8243 - Strong performance but behind Random Forest and XGBoost.\n",
        "\n",
        "**Visual Interpretation:**\n",
        "\n",
        "- **ROC Curve**: Close to the top-left but dips slightly due to lower discrimination.\n",
        "- **Precision-Recall Curve**: Good recall, but precision limitations reduce overall performance.\n",
        "- **Confusion Matrix**: Indicates effective fraud detection but misclassifies more legitimate transactions.\n",
        "\n",
        "**Summary:** The Neural Network performs well in identifying fraud (high recall) but struggles with precision, making it less ideal where false positives are costly. Further tuning could improve its balance.\n",
        "\n",
        "\n",
        "#### **5. Voting Classifier (Random Forest + XGBoost)**\n",
        "\n",
        "**Performance Metrics:**\n",
        "\n",
        "- **Precision for fraud**: 0.80 - Matches Random Forest.\n",
        "- **Recall for fraud**: 0.87 - A balanced performance, slightly behind Random Forest.\n",
        "- **F1-Score**: 0.83 - Reflects a well-balanced model.\n",
        "- **ROC-AUC Score**: 0.9912 - Marginally better than both Random Forest and XGBoost alone.\n",
        "- **AUPRC Score**: 0.8812 - Comparable to Random Forest and XGBoost, ensuring robust fraud detection.\n",
        "\n",
        "**Visual Interpretation:**\n",
        "\n",
        "- **ROC Curve**: Matches or slightly outperforms the individual models.\n",
        "- **Precision-Recall Curve**: Consistently high, reflecting a balance of precision and recall.\n",
        "- **Confusion Matrix**: Maintains minimal false positives and negatives, similar to its component models.\n",
        "\n",
        "**Summary:** The Voting Classifier leverages the strengths of Random Forest and XGBoost, offering balanced and robust performance. It is a strong choice for fraud detection, mitigating individual model weaknesses.\n",
        "\n",
        "---\n",
        "\n",
        "### Final Recommendations\n",
        "\n",
        "1. **Best Model for Fraud Detection**:\n",
        "    \n",
        "    - Random Forest and XGBoost offer strong precision, recall, and F1-scores with high AUPRC and ROC-AUC. They are ideal for fraud detection, though their performance should be validated further.\n",
        "2. **Balanced Choice**:\n",
        "    \n",
        "    - Logistic Regression is a simpler, reliable option with high recall and good overall performance. However, its low precision could cause operational challenges.\n",
        "3. **For High Recall**:\n",
        "    \n",
        "    - Neural Network is suitable if recall is the primary focus. However, its low precision makes it less ideal in scenarios where false positives must be minimized.\n",
        "4. **Ensemble Approach**:\n",
        "    \n",
        "    - The Voting Classifier combines the robustness of Random Forest and XGBoost, achieving balanced performance with minimal overfitting risks. It is the most reliable choice for production deployment.\n",
        "\n"
      ],
      "metadata": {
        "id": "LosIcoslOudC"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}